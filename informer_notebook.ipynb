{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('thesis': conda)"
  },
  "interpreter": {
   "hash": "75368328311c6b7cecdd65df4abe52d34ef65e56f3beb623598d89faa2b2621e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Informer Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libs\n",
    "\n",
    "import math\n",
    "from math import e\n",
    "import os\n",
    "from operator import mod\n",
    "from datetime import datetime, timedelta\n",
    "from sys import platform\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn.modules import transformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from runx.logx import logx\n",
    "import pickle\n",
    "from typing import List\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pandas.tseries import offsets\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# libs\n",
    "import evaluate\n",
    "import libs.utils as utils\n",
    "from libs.losses import LossHelper\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "args = utils.DotDict()\n",
    "args.arch = \"informer\"\n",
    "args.loss_type = \"sharpe\"\n",
    "args.stopping_type = \"strategy\"\n",
    "args.filename = \"futures_prop.csv\"\n",
    "args.frequency = \"d\"\n",
    "args.start_date = \"01/01/1990\" # do not change\n",
    "args.test_date = \"01/01/2000\" # do not change\n",
    "args.end_date = \"01/01/2005\" # do not change\n",
    "args.scaler = \"none\" # do not change\n",
    "args.lead_tages = \"1\" # do not change\n",
    "args.win_len = 63\n",
    "args.step = 63\n",
    "args.epochs = 100\n",
    "args.patience = 25\n",
    "args.lr = 0.01\n",
    "args.batch_size = 128\n",
    "args.max_grad_norm = 1\n",
    "args.dropout = 0.2\n",
    "args.n_layer = 2\n",
    "args.d_hidden = 64\n",
    "args.d_model = 32\n",
    "args.n_head = 4\n",
    "args.attn = 'prob'\n",
    "args.embed_type = 'timeF'\n",
    "args.factor = 5\n",
    "args.do_log = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "tmp = pickle.load(open(\"data_tmp.p\", \"rb\"))\n",
    "train_iter = tmp['train_iter']\n",
    "val_iter = tmp['val_iter']\n",
    "val_df = tmp['val_df']\n",
    "test_iter = tmp['test_iter']\n",
    "test_df = tmp['test_df']\n",
    "\n",
    "loss_type = LossHelper.get_loss_type(args.loss_type)\n",
    "train_manager = {\n",
    "    # args\n",
    "    'args': args if type(args) == utils.DotDict else vars(args),\n",
    "    # loss\n",
    "    'loss_label': args.loss_type,\n",
    "    'loss_type': loss_type,\n",
    "    'loss_fn': LossHelper.get_loss_function(loss_type),\n",
    "    # learning\n",
    "    'lr': args.lr,\n",
    "    'patience': args.patience,\n",
    "    'epochs': args.epochs,\n",
    "    # data\n",
    "    'frequency': args.frequency,\n",
    "    'year_test': pd.to_datetime(args.test_date).year,\n",
    "    # scaler\n",
    "    'scaler_path': None\n",
    "}"
   ]
  },
  {
   "source": [
    "## Informer model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### I.1 Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        # See https://github.com/zhouhaoyi/Informer2020/issues/149\n",
    "        if freq == 'h':\n",
    "            self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        # See https://github.com/zhouhaoyi/Informer2020/issues/149\n",
    "        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(\n",
    "            self, 'hour_embed') else 0.\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return minute_x + hour_x + weekday_x + day_x + month_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='d', dropout=0.1, only_encoder=False):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.only_encoder = only_encoder\n",
    "\n",
    "        # SVEN\n",
    "        if self.only_encoder:\n",
    "            print(\"> Informer only encoding\")\n",
    "            self.value_embedding = nn.Linear(c_in, d_model)\n",
    "        else:\n",
    "            self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        # SVEN\n",
    "        x = self.value_embedding(\n",
    "            x) + self.position_embedding(x_mark) + self.temporal_embedding(x_mark)\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- --- ---\n",
    "# informer_encoder.py\n",
    "# Sven Giegerich / 19.05.2021\n",
    "# --- --- ---\n",
    "\n",
    "# Based on,\n",
    "# Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H. and Zhang, W., 2020.\n",
    "# Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting.\n",
    "# arXiv preprint arXiv:2012.07436.\n",
    "\n",
    "\n",
    "# Notebook\n",
    "#from libs.models.embeddings import *\n",
    "\n",
    "# --- ---\n",
    "# ENCODER ONLY\n",
    "# model.py\n",
    "# --- ---\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "\n",
    "    name = 'informer'\n",
    "    batch_first = True\n",
    "\n",
    "    def __init__(self, enc_in, c_out, loss_type,\n",
    "                 factor=5, d_model=512, n_heads=8, e_layers=3, d_ff=512,\n",
    "                 dropout=0.0, attn='prob', embed_type='fixed', freq='d', activation='gelu',\n",
    "                 output_attention=True, distil=False, win_len=None):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "\n",
    "        self.enc_in = enc_in\n",
    "        self.c_out = c_out\n",
    "        self.factor = factor\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.e_layers = e_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.freq = freq\n",
    "        self.attn = attn\n",
    "        self.output_attention = output_attention\n",
    "        self.win_len = win_len\n",
    "        self.embed_type = embed_type  # either 'fixed' or 'timeF'\n",
    "        self.distil = distil\n",
    "\n",
    "        if self.win_len is not None:\n",
    "            self.enc_self_mask = self.generate_causal_mask(win_len)\n",
    "\n",
    "        mask_flag = True\n",
    "\n",
    "        # Embedding\n",
    "        if self.embed_type == \"simple\":\n",
    "            print(\"> Use simple positional encoding\")\n",
    "            self.enc_simple_embedding = nn.Linear(enc_in, d_model)\n",
    "            self.pos_encoder = SimplePositionalEncoding(d_model=d_model)\n",
    "        else:\n",
    "            print(\"> Use data embedding\")\n",
    "            self.enc_data_embedding = DataEmbedding(\n",
    "                c_in=enc_in, d_model=d_model, embed_type=embed_type, freq=freq, dropout=dropout, only_encoder=True)\n",
    "\n",
    "        # Attention\n",
    "        Attn = ProbCausalAttention if attn == 'prob' else FullAttention\n",
    "\n",
    "        print(f\"> Using attention mechanism {Attn}\")\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(Attn(mask_flag=mask_flag, factor=factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                                   d_model, n_heads, mix=False),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(\n",
    "                    d_model\n",
    "                ) for l in range(e_layers-1)\n",
    "            ] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        # SVEN\n",
    "        # Decoder (aka projection)\n",
    "        self.decoder = nn.Linear(d_model, c_out, bias=True)\n",
    "        self.output_fn = LossHelper.get_output_activation(loss_type)\n",
    "\n",
    "    def forward(self, src, x_mark_enc, enc_self_mask=None):\n",
    "        if self.embed_type == \"simple\":\n",
    "            # opt1: simple pos encoding (original attention is all you need)\n",
    "            emb = self.pos_encoder(self.enc_simple_embedding(src))\n",
    "        else:\n",
    "            # opt2:\n",
    "            emb = self.enc_data_embedding(src, x_mark_enc)\n",
    "\n",
    "        enc_out, attns = self.encoder(emb, attn_mask=enc_self_mask)\n",
    "\n",
    "        # SVEN: leave out the decoder part\n",
    "        dec_out = self.decoder(enc_out)\n",
    "        out = self.output_fn(dec_out)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return out, attns\n",
    "        else:\n",
    "            return out  # [B, L, D]\n",
    "\n",
    "    def generate_causal_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\n",
    "            '-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "# --- ---\n",
    "# encoder.py\n",
    "# --- ---\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
    "                                  out_channels=c_in,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=2,\n",
    "                                  padding_mode='circular')\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model,\n",
    "                               out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        # x = x + self.dropout(self.attention(\n",
    "        #     x, x, x,\n",
    "        #     attn_mask = attn_mask\n",
    "        # ))\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x+y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, encoders, inp_lens):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.inp_lens = inp_lens\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        x_stack = []\n",
    "        attns = []\n",
    "        for i_len, encoder in zip(self.inp_lens, self.encoders):\n",
    "            inp_len = x.shape[1]//(2**i_len)\n",
    "            x_s, attn = encoder(x[:, -inp_len:, :])\n",
    "            x_stack.append(x_s)\n",
    "            attns.append(attn)\n",
    "        x_stack = torch.cat(x_stack, -2)\n",
    "\n",
    "        return x_stack, attns\n",
    "\n",
    "# --- ---\n",
    "# decoder.py\n",
    "# --- ---\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model,\n",
    "                               out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x+y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# --- ---\n",
    "# embed.py\n",
    "# --- ---\n",
    "\n",
    "# see seperate file libs.models.embeddings.py\n",
    "\n",
    "# --- ---\n",
    "# attn.py\n",
    "# --- ---\n",
    "\n",
    "\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1./sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class ProbCausalAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbCausalAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        # real U = U_part(factor*ln(L_k))*L_q\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(\n",
    "            L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(\n",
    "            Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]  # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2, 1)\n",
    "        keys = keys.transpose(2, 1)\n",
    "        values = values.transpose(2, 1)\n",
    "\n",
    "        U_part = self.factor * \\\n",
    "            np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n",
    "        u = self.factor * \\\n",
    "            np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n",
    "\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "\n",
    "        scores_top, index = self._prob_QK(\n",
    "            queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "\n",
    "        # NEW: START ----\n",
    "\n",
    "        # uniform distr as starter\n",
    "        scores = (torch.ones([B, H, L_Q, L_Q]) /\n",
    "                  L_Q).type_as(scores_top).to(device)\n",
    "        # ... uniform\n",
    "        scores[torch.arange(B)[:, None, None],\n",
    "               torch.arange(H)[None, :, None],\n",
    "               index, :] = scores_top\n",
    "\n",
    "        # causal mask\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L_Q, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        # tmp: plus dropout? not in prob, but in full... don't think necessary\n",
    "        A = torch.softmax(scale * scores, dim=-1)\n",
    "        V = torch.einsum(\"bhls,bhsd->blhd\", A, values)\n",
    "\n",
    "        # NEW: END ----\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "        pass\n",
    "\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        # real U = U_part(factor*ln(L_k))*L_q\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(\n",
    "            L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(\n",
    "            Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]  # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if not self.mask_flag:\n",
    "            # V_sum = V.sum(dim=-2)\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H,\n",
    "                                                L_Q, V_sum.shape[-1]).clone()\n",
    "        else:  # use mask\n",
    "            # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            assert(L_Q == L_V)\n",
    "            contex = V.cumsum(dim=-2)\n",
    "\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)  # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            # SVEN: add uniform distribution\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) /\n",
    "                     L_V).type_as(attn).to(attn.device)\n",
    "\n",
    "            # SVEN: probably just for visz. -> more substantive for context\n",
    "            # ... add mask here!\n",
    "            # ... but also necessary somwhere else? as this only effects display\n",
    "            causal_mask = TriangularCausalMask(\n",
    "                B, L_Q, device).mask[0, :, :].squeeze()\n",
    "            attns.masked_fill_(causal_mask, 0)  # broadcasting\n",
    "            # END SVEN\n",
    "\n",
    "            # SVEN: add the actual attention values for the sparse entries\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[\n",
    "                None, :, None], index, :] = attn\n",
    "            return (context_in, attns)\n",
    "        else:\n",
    "            return (context_in, None)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2, 1)\n",
    "        keys = keys.transpose(2, 1)\n",
    "        values = values.transpose(2, 1)\n",
    "\n",
    "        U_part = self.factor * \\\n",
    "            np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n",
    "        u = self.factor * \\\n",
    "            np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n",
    "\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "\n",
    "        scores_top, index = self._prob_QK(\n",
    "            queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        # update the context with selected top_k queries\n",
    "        context, attn = self._update_context(\n",
    "            context, values, scores_top, index, L_Q, attn_mask)\n",
    "\n",
    "        return context.transpose(2, 1).contiguous(), attn\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads,\n",
    "                 d_keys=None, d_values=None, mix=False):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        self.mix = mix\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        if self.mix:\n",
    "            out = out.transpose(2, 1).contiguous()\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "# --- ---\n",
    "# utils masking\n",
    "# --- ---\n",
    "\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(\n",
    "                mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class ProbMask():\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(\n",
    "            L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self._mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "# --- ---\n",
    "# utils timefeatures\n",
    "# --- ---\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, timeenc=1, freq='h'):\n",
    "    \"\"\"\n",
    "    > `time_features` takes in a `dates` dataframe with a 'dates' column and extracts the date down to `freq` where freq can be any of the following if `timeenc` is 0: \n",
    "    > * m - [month]\n",
    "    > * w - [month]\n",
    "    > * d - [month, day, weekday]\n",
    "    > * b - [month, day, weekday]\n",
    "    > * h - [month, day, weekday, hour]\n",
    "    > * t - [month, day, weekday, hour, *minute]\n",
    "    > \n",
    "    > If `timeenc` is 1, a similar, but different list of `freq` values are supported (all encoded between [-0.5 and 0.5]): \n",
    "    > * Q - [month]\n",
    "    > * M - [month]\n",
    "    > * W - [Day of month, week of year]\n",
    "    > * D - [Day of week, day of month, day of year]\n",
    "    > * B - [Day of week, day of month, day of year]\n",
    "    > * H - [Hour of day, day of week, day of month, day of year]\n",
    "    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n",
    "    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n",
    "\n",
    "    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n",
    "    \"\"\"\n",
    "    if timeenc == 0:\n",
    "        dates['month'] = dates.date.apply(lambda row: row.month, 1)\n",
    "        dates['day'] = dates.date.apply(lambda row: row.day, 1)\n",
    "        dates['weekday'] = dates.date.apply(lambda row: row.weekday(), 1)\n",
    "        dates['hour'] = dates.date.apply(lambda row: row.hour, 1)\n",
    "        dates['minute'] = dates.date.apply(lambda row: row.minute, 1)\n",
    "        dates['minute'] = dates.minute.map(lambda x: x//15)\n",
    "        freq_map = {\n",
    "            'y': [], 'm': ['month'], 'w': ['month'], 'd': ['month', 'day', 'weekday'],\n",
    "            'b': ['month', 'day', 'weekday'], 'h': ['month', 'day', 'weekday', 'hour'],\n",
    "            't': ['month', 'day', 'weekday', 'hour', 'minute'],\n",
    "        }\n",
    "        return dates[freq_map[freq.lower()]].values\n",
    "    if timeenc == 1:\n",
    "        dates = pd.to_datetime(dates.date.values)\n",
    "        return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)]).transpose(1, 0)"
   ]
  },
  {
   "source": [
    "# Run "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Build model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3) Build model: informer\n> Use data embedding\n> Informer only encoding\n> Using attention mechanism <class '__main__.ProbCausalAttention'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"(3) Build model: {args.arch}\")\n",
    "d_input = 8\n",
    "d_output = 1\n",
    "\n",
    "freq = 'd'  # daily\n",
    "factor = args.factor  # factor to sample for prob attention\n",
    "d_ff = args.d_model\n",
    "attn = args.attn  # 'full' or 'prob'\n",
    "embed_type = args.embed_type  # could be changed to learnable\n",
    "# if n_layer > 1: each succ layer will be reduced by 2\n",
    "do_distil = False\n",
    "output_attention = True\n",
    "win_len = args.win_len\n",
    "\n",
    "model = InformerEncoder(enc_in=d_input, c_out=d_output, factor=factor,loss_type=train_manager['loss_type'], d_model=args.d_model, n_heads=args.n_head,e_layers=args.n_layer, d_ff=d_ff, dropout=args.dropout, attn=attn, embed_type=embed_type,freq=freq, output_attention=output_attention, distil=do_distil, win_len=win_len)"
   ]
  },
  {
   "source": [
    "## Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, val_iter, train_manager, do_log=False, val_df=None):\n",
    "    model = model.to(device).double()\n",
    "    best_val_score = np.inf\n",
    "\n",
    "    # train manager ----\n",
    "    train_manager['optimizer'] = torch.optim.Adam(\n",
    "        model.parameters(), lr=train_manager['lr'])\n",
    "\n",
    "    early_stopping = utils.EarlyStopping(\n",
    "        patience=train_manager['patience'], path=None, verbose=True)\n",
    "\n",
    "    # run training ----\n",
    "    for epoch_i in range(train_manager['epochs']):\n",
    "        epoch_loss = run_epoch(\n",
    "            model=model, train_iter=train_iter, train_manager=train_manager, epoch_i=epoch_i, do_log=do_log)\n",
    "\n",
    "        val_loss = evaluate_iter(\n",
    "            model=model, data_iter=val_iter, train_manager=train_manager)\n",
    "        val_str_loss = evaluate_iter(\n",
    "            model=model, data_iter=val_iter, train_manager=train_manager, do_strategy=True, base_df=val_df)\n",
    "\n",
    "        if val_loss < best_val_score:\n",
    "            best_val_score = val_loss\n",
    "\n",
    "        # verb ----\n",
    "        epoch_print = f\">> Train Epoch {epoch_i + 1}\\t -- avg --\\t train batch loss: {epoch_loss:.6f}\\t val batch loss: {val_loss:.6f} \\t  val strategy loss: {val_str_loss:.6f}\"\n",
    "        if do_log:\n",
    "            logx.msg(epoch_print)\n",
    "            logx.add_scalar(\"Loss/val\", val_loss, epoch_i)\n",
    "\n",
    "            metrics_train = {'loss': epoch_loss}\n",
    "            metrics_val = {'loss': val_loss}\n",
    "            # to be extented\n",
    "            logx.metric(phase='train', metrics=metrics_train,\n",
    "                        epoch=epoch_i + 1)\n",
    "            logx.metric(phase='val', metrics=metrics_val,\n",
    "                        epoch=epoch_i + 1)\n",
    "        else:\n",
    "            print(epoch_print)\n",
    "\n",
    "        # early stopping ----\n",
    "        if train_manager['args']['stopping_type'] == 'strategy':\n",
    "            checkpoint_loss = val_str_loss\n",
    "        else:\n",
    "            checkpoint_loss = val_loss\n",
    "\n",
    "        early_stopping(checkpoint_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"> Early stopping\")\n",
    "            break\n",
    "\n",
    "    best_val_loss = -early_stopping.best_score\n",
    "    return (early_stopping.path, best_val_loss)\n",
    "\n",
    "\n",
    "def run_epoch(model, train_iter, train_manager, epoch_i=None, do_log=False):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = train_manager['optimizer']\n",
    "    loss_fn = train_manager['loss_fn']\n",
    "    max_grad_norm = train_manager['args']['max_grad_norm']\n",
    "\n",
    "    loss_epoch = np.zeros((len(train_iter), 2))  # batch loss & batch size\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        inputs = batch['inp'].double().to(device)\n",
    "        labels = batch['trg'].double().to(device)\n",
    "        returns = batch['rts'].double().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # time embedding?\n",
    "        if model.name == 'informer':\n",
    "            inputs_time_embd = batch['time_embd'].double().to(device)\n",
    "            prediction, attns = model(inputs, inputs_time_embd)\n",
    "        else:\n",
    "            prediction = model(inputs)\n",
    "\n",
    "        if LossHelper.use_returns_for_loss(train_manager['loss_type']):\n",
    "            loss = loss_fn(prediction, returns,\n",
    "                           freq=train_manager['frequency'])\n",
    "        else:\n",
    "            loss = loss_fn(prediction, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = batch['inp'].shape[0]\n",
    "        loss_epoch[i] = np.array([loss, batch_size])\n",
    "\n",
    "        # log results\n",
    "        if epoch_i is not None and i % 5 == 0:\n",
    "            if do_log:\n",
    "                writer_path = f\"Loss/train/{train_manager['loss_label']}/{train_manager['year_test']}\"\n",
    "                logx.add_scalar(writer_path, loss, epoch_i *\n",
    "                                len(train_iter) + i)\n",
    "            print_msg = f\">> Train Epoch {epoch_i+1}\\t batch {i}\\t train batch loss: {loss:.6f}\"\n",
    "            if do_log:\n",
    "                logx.msg(print_msg)\n",
    "            else:\n",
    "                print(print_msg)\n",
    "\n",
    "    mean_loss_epoch = np.average(loss_epoch[:, 0], weights=loss_epoch[:, 1])\n",
    "    return mean_loss_epoch\n",
    "\n",
    "\n",
    "def evaluate_iter(model, data_iter, train_manager, do_strategy=False, base_df=None, do_log=False):\n",
    "    if do_strategy and base_df is not None:\n",
    "        # strategy loss ----\n",
    "        df_skeleton = base_df.swaplevel(axis=1)['prs']\n",
    "        scaled_rts = base_df.xs('rts_scaled', axis=1,\n",
    "                                level=1, drop_level=True)\n",
    "\n",
    "        predictions = evaluate.calc_predictions_df(model, data_iter, df_shape=df_skeleton.shape,\n",
    "                                                   df_index=df_skeleton.index, df_insts=df_skeleton.columns,\n",
    "                                                   win_step=train_manager['args']['win_len'], scaler=train_manager['args']['scaler'], loss_type=train_manager['loss_type'])\n",
    "        positions = evaluate.calc_position_df(\n",
    "            predictions, train_manager['loss_type'])\n",
    "        str_returns = utils.calc_strategy_returns(\n",
    "            positions=positions, realized_returns=scaled_rts, aggregate_by='time', lead=1)\n",
    "\n",
    "        loss_fn = LossHelper.get_strategy_loss_function(\n",
    "            train_manager['loss_type'])\n",
    "        str_loss = loss_fn(str_returns)\n",
    "\n",
    "        return str_loss\n",
    "    else:\n",
    "        # batch loss ----\n",
    "        return evaluate.evaluate_model(model, data_iter, train_manager, do_log=do_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Train Epoch 1\t batch 0\t train batch loss: 0.148552\n",
      ">> Train Epoch 1\t -- avg --\t train batch loss: -0.451244\t val batch loss: -0.377727 \t  val strategy loss: -1.493124\n",
      ">> Train Epoch 2\t batch 0\t train batch loss: -0.613084\n",
      ">> Train Epoch 2\t -- avg --\t train batch loss: -0.797094\t val batch loss: -0.297252 \t  val strategy loss: -1.205903\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 3\t batch 0\t train batch loss: -0.765849\n",
      ">> Train Epoch 3\t -- avg --\t train batch loss: -0.827781\t val batch loss: -0.223587 \t  val strategy loss: -0.866911\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 4\t batch 0\t train batch loss: -0.770731\n",
      ">> Train Epoch 4\t -- avg --\t train batch loss: -0.920287\t val batch loss: -0.239677 \t  val strategy loss: -1.052359\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 5\t batch 0\t train batch loss: -1.156718\n",
      ">> Train Epoch 5\t -- avg --\t train batch loss: -1.006408\t val batch loss: -0.239015 \t  val strategy loss: -1.084095\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 6\t batch 0\t train batch loss: -1.286376\n",
      ">> Train Epoch 6\t -- avg --\t train batch loss: -1.139184\t val batch loss: -0.179672 \t  val strategy loss: -0.624562\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 7\t batch 0\t train batch loss: -0.952056\n",
      ">> Train Epoch 7\t -- avg --\t train batch loss: -1.195583\t val batch loss: -0.243494 \t  val strategy loss: -0.908638\n",
      "EarlyStopping counter: 6 out of 25\n",
      ">> Train Epoch 8\t batch 0\t train batch loss: -1.124604\n",
      ">> Train Epoch 8\t -- avg --\t train batch loss: -1.117482\t val batch loss: -0.249074 \t  val strategy loss: -1.169142\n",
      "EarlyStopping counter: 7 out of 25\n",
      ">> Train Epoch 9\t batch 0\t train batch loss: -1.337272\n",
      ">> Train Epoch 9\t -- avg --\t train batch loss: -1.233956\t val batch loss: -0.272095 \t  val strategy loss: -1.026582\n",
      "EarlyStopping counter: 8 out of 25\n",
      ">> Train Epoch 10\t batch 0\t train batch loss: -1.261605\n",
      ">> Train Epoch 10\t -- avg --\t train batch loss: -1.353253\t val batch loss: -0.422090 \t  val strategy loss: -1.918498\n",
      ">> Train Epoch 11\t batch 0\t train batch loss: -1.584386\n",
      ">> Train Epoch 11\t -- avg --\t train batch loss: -1.320894\t val batch loss: -0.478946 \t  val strategy loss: -2.004378\n",
      ">> Train Epoch 12\t batch 0\t train batch loss: -1.315700\n",
      ">> Train Epoch 12\t -- avg --\t train batch loss: -1.443166\t val batch loss: -0.449605 \t  val strategy loss: -1.748360\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 13\t batch 0\t train batch loss: -1.378176\n",
      ">> Train Epoch 13\t -- avg --\t train batch loss: -1.327464\t val batch loss: -0.153931 \t  val strategy loss: -0.678797\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 14\t batch 0\t train batch loss: -1.532949\n",
      ">> Train Epoch 14\t -- avg --\t train batch loss: -1.356750\t val batch loss: -0.185997 \t  val strategy loss: -0.775089\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 15\t batch 0\t train batch loss: -1.563205\n",
      ">> Train Epoch 15\t -- avg --\t train batch loss: -1.412626\t val batch loss: -0.251122 \t  val strategy loss: -0.595737\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 16\t batch 0\t train batch loss: -1.516102\n",
      ">> Train Epoch 16\t -- avg --\t train batch loss: -1.487681\t val batch loss: -0.301459 \t  val strategy loss: -1.237446\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 17\t batch 0\t train batch loss: -1.424273\n",
      ">> Train Epoch 17\t -- avg --\t train batch loss: -1.459726\t val batch loss: -0.565330 \t  val strategy loss: -2.270101\n",
      ">> Train Epoch 18\t batch 0\t train batch loss: -1.789347\n",
      ">> Train Epoch 18\t -- avg --\t train batch loss: -1.560755\t val batch loss: -0.448054 \t  val strategy loss: -2.151616\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 19\t batch 0\t train batch loss: -1.668998\n",
      ">> Train Epoch 19\t -- avg --\t train batch loss: -1.610898\t val batch loss: -0.524197 \t  val strategy loss: -2.592097\n",
      ">> Train Epoch 20\t batch 0\t train batch loss: -1.812418\n",
      ">> Train Epoch 20\t -- avg --\t train batch loss: -1.594498\t val batch loss: -0.581270 \t  val strategy loss: -2.474587\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 21\t batch 0\t train batch loss: -1.598304\n",
      ">> Train Epoch 21\t -- avg --\t train batch loss: -1.640173\t val batch loss: -0.685987 \t  val strategy loss: -2.819188\n",
      ">> Train Epoch 22\t batch 0\t train batch loss: -1.719769\n",
      ">> Train Epoch 22\t -- avg --\t train batch loss: -1.727977\t val batch loss: -0.459968 \t  val strategy loss: -2.521342\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 23\t batch 0\t train batch loss: -1.151849\n",
      ">> Train Epoch 23\t -- avg --\t train batch loss: -1.511253\t val batch loss: -0.690720 \t  val strategy loss: -3.591265\n",
      ">> Train Epoch 24\t batch 0\t train batch loss: -1.608270\n",
      ">> Train Epoch 24\t -- avg --\t train batch loss: -1.711903\t val batch loss: -0.875294 \t  val strategy loss: -3.481955\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 25\t batch 0\t train batch loss: -2.036714\n",
      ">> Train Epoch 25\t -- avg --\t train batch loss: -1.909308\t val batch loss: -0.968342 \t  val strategy loss: -4.205113\n",
      ">> Train Epoch 26\t batch 0\t train batch loss: -1.748333\n",
      ">> Train Epoch 26\t -- avg --\t train batch loss: -1.800375\t val batch loss: -0.861024 \t  val strategy loss: -3.681148\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 27\t batch 0\t train batch loss: -2.128352\n",
      ">> Train Epoch 27\t -- avg --\t train batch loss: -1.936603\t val batch loss: -0.944139 \t  val strategy loss: -3.462860\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 28\t batch 0\t train batch loss: -1.534321\n",
      ">> Train Epoch 28\t -- avg --\t train batch loss: -1.758794\t val batch loss: -0.925768 \t  val strategy loss: -4.405573\n",
      ">> Train Epoch 29\t batch 0\t train batch loss: -2.367114\n",
      ">> Train Epoch 29\t -- avg --\t train batch loss: -2.002958\t val batch loss: -0.878862 \t  val strategy loss: -3.483023\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 30\t batch 0\t train batch loss: -1.679454\n",
      ">> Train Epoch 30\t -- avg --\t train batch loss: -1.924585\t val batch loss: -0.939798 \t  val strategy loss: -3.884489\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 31\t batch 0\t train batch loss: -2.059856\n",
      ">> Train Epoch 31\t -- avg --\t train batch loss: -2.079930\t val batch loss: -1.009229 \t  val strategy loss: -3.832299\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 32\t batch 0\t train batch loss: -1.852946\n",
      ">> Train Epoch 32\t -- avg --\t train batch loss: -2.000203\t val batch loss: -0.965331 \t  val strategy loss: -3.934890\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 33\t batch 0\t train batch loss: -1.814253\n",
      ">> Train Epoch 33\t -- avg --\t train batch loss: -1.943600\t val batch loss: -0.795329 \t  val strategy loss: -3.751626\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 34\t batch 0\t train batch loss: -2.163376\n",
      ">> Train Epoch 34\t -- avg --\t train batch loss: -2.081231\t val batch loss: -0.753339 \t  val strategy loss: -3.964004\n",
      "EarlyStopping counter: 6 out of 25\n",
      ">> Train Epoch 35\t batch 0\t train batch loss: -2.390349\n",
      ">> Train Epoch 35\t -- avg --\t train batch loss: -2.291259\t val batch loss: -1.023949 \t  val strategy loss: -4.683654\n",
      ">> Train Epoch 36\t batch 0\t train batch loss: -2.187805\n",
      ">> Train Epoch 36\t -- avg --\t train batch loss: -2.136137\t val batch loss: -1.085627 \t  val strategy loss: -5.413066\n",
      ">> Train Epoch 37\t batch 0\t train batch loss: -2.546962\n",
      ">> Train Epoch 37\t -- avg --\t train batch loss: -2.068201\t val batch loss: -1.376936 \t  val strategy loss: -5.839777\n",
      ">> Train Epoch 38\t batch 0\t train batch loss: -2.125953\n",
      ">> Train Epoch 38\t -- avg --\t train batch loss: -2.240136\t val batch loss: -1.328632 \t  val strategy loss: -5.830716\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 39\t batch 0\t train batch loss: -2.109008\n",
      ">> Train Epoch 39\t -- avg --\t train batch loss: -2.150334\t val batch loss: -1.315910 \t  val strategy loss: -5.692033\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 40\t batch 0\t train batch loss: -2.365324\n",
      ">> Train Epoch 40\t -- avg --\t train batch loss: -2.283786\t val batch loss: -1.285583 \t  val strategy loss: -5.767847\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 41\t batch 0\t train batch loss: -2.406449\n",
      ">> Train Epoch 41\t -- avg --\t train batch loss: -2.176525\t val batch loss: -0.972204 \t  val strategy loss: -4.262962\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 42\t batch 0\t train batch loss: -2.110725\n",
      ">> Train Epoch 42\t -- avg --\t train batch loss: -1.993701\t val batch loss: -1.073568 \t  val strategy loss: -5.179747\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 43\t batch 0\t train batch loss: -2.135487\n",
      ">> Train Epoch 43\t -- avg --\t train batch loss: -2.209227\t val batch loss: -1.080441 \t  val strategy loss: -6.006864\n",
      ">> Train Epoch 44\t batch 0\t train batch loss: -2.113229\n",
      ">> Train Epoch 44\t -- avg --\t train batch loss: -2.342526\t val batch loss: -1.236175 \t  val strategy loss: -5.147937\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 45\t batch 0\t train batch loss: -2.449710\n",
      ">> Train Epoch 45\t -- avg --\t train batch loss: -2.277172\t val batch loss: -0.938854 \t  val strategy loss: -4.591669\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 46\t batch 0\t train batch loss: -1.821674\n",
      ">> Train Epoch 46\t -- avg --\t train batch loss: -1.958670\t val batch loss: -0.900490 \t  val strategy loss: -3.787432\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 47\t batch 0\t train batch loss: -2.204459\n",
      ">> Train Epoch 47\t -- avg --\t train batch loss: -2.084359\t val batch loss: -1.133929 \t  val strategy loss: -3.848425\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 48\t batch 0\t train batch loss: -1.962575\n",
      ">> Train Epoch 48\t -- avg --\t train batch loss: -2.134585\t val batch loss: -1.216525 \t  val strategy loss: -5.431627\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 49\t batch 0\t train batch loss: -2.252590\n",
      ">> Train Epoch 49\t -- avg --\t train batch loss: -2.193557\t val batch loss: -1.247980 \t  val strategy loss: -5.486087\n",
      "EarlyStopping counter: 6 out of 25\n",
      ">> Train Epoch 50\t batch 0\t train batch loss: -2.501207\n",
      ">> Train Epoch 50\t -- avg --\t train batch loss: -2.301229\t val batch loss: -0.882439 \t  val strategy loss: -4.310648\n",
      "EarlyStopping counter: 7 out of 25\n",
      ">> Train Epoch 51\t batch 0\t train batch loss: -2.227116\n",
      ">> Train Epoch 51\t -- avg --\t train batch loss: -2.306935\t val batch loss: -1.191182 \t  val strategy loss: -4.881286\n",
      "EarlyStopping counter: 8 out of 25\n",
      ">> Train Epoch 52\t batch 0\t train batch loss: -2.295118\n",
      ">> Train Epoch 52\t -- avg --\t train batch loss: -2.224259\t val batch loss: -1.061857 \t  val strategy loss: -5.028716\n",
      "EarlyStopping counter: 9 out of 25\n",
      ">> Train Epoch 53\t batch 0\t train batch loss: -1.902973\n",
      ">> Train Epoch 53\t -- avg --\t train batch loss: -2.208962\t val batch loss: -1.083260 \t  val strategy loss: -4.149224\n",
      "EarlyStopping counter: 10 out of 25\n",
      ">> Train Epoch 54\t batch 0\t train batch loss: -2.280897\n",
      ">> Train Epoch 54\t -- avg --\t train batch loss: -2.019433\t val batch loss: -0.691602 \t  val strategy loss: -2.728277\n",
      "EarlyStopping counter: 11 out of 25\n",
      ">> Train Epoch 55\t batch 0\t train batch loss: -2.148531\n",
      ">> Train Epoch 55\t -- avg --\t train batch loss: -2.200755\t val batch loss: -1.000102 \t  val strategy loss: -4.551816\n",
      "EarlyStopping counter: 12 out of 25\n",
      ">> Train Epoch 56\t batch 0\t train batch loss: -2.175259\n",
      ">> Train Epoch 56\t -- avg --\t train batch loss: -2.208696\t val batch loss: -1.174374 \t  val strategy loss: -4.560445\n",
      "EarlyStopping counter: 13 out of 25\n",
      ">> Train Epoch 57\t batch 0\t train batch loss: -2.281387\n",
      ">> Train Epoch 57\t -- avg --\t train batch loss: -2.405589\t val batch loss: -1.185123 \t  val strategy loss: -5.668814\n",
      "EarlyStopping counter: 14 out of 25\n",
      ">> Train Epoch 58\t batch 0\t train batch loss: -2.386364\n",
      ">> Train Epoch 58\t -- avg --\t train batch loss: -2.372427\t val batch loss: -1.191122 \t  val strategy loss: -5.808324\n",
      "EarlyStopping counter: 15 out of 25\n",
      ">> Train Epoch 59\t batch 0\t train batch loss: -2.518440\n",
      ">> Train Epoch 59\t -- avg --\t train batch loss: -2.582567\t val batch loss: -1.354614 \t  val strategy loss: -6.441751\n",
      ">> Train Epoch 60\t batch 0\t train batch loss: -2.567230\n",
      ">> Train Epoch 60\t -- avg --\t train batch loss: -2.362733\t val batch loss: -1.142406 \t  val strategy loss: -5.218815\n",
      "EarlyStopping counter: 1 out of 25\n",
      ">> Train Epoch 61\t batch 0\t train batch loss: -2.491978\n",
      ">> Train Epoch 61\t -- avg --\t train batch loss: -2.455575\t val batch loss: -1.207982 \t  val strategy loss: -5.083599\n",
      "EarlyStopping counter: 2 out of 25\n",
      ">> Train Epoch 62\t batch 0\t train batch loss: -2.386098\n",
      ">> Train Epoch 62\t -- avg --\t train batch loss: -2.427856\t val batch loss: -1.238856 \t  val strategy loss: -5.431781\n",
      "EarlyStopping counter: 3 out of 25\n",
      ">> Train Epoch 63\t batch 0\t train batch loss: -2.505785\n",
      ">> Train Epoch 63\t -- avg --\t train batch loss: -2.194330\t val batch loss: -1.067996 \t  val strategy loss: -5.674942\n",
      "EarlyStopping counter: 4 out of 25\n",
      ">> Train Epoch 64\t batch 0\t train batch loss: -2.652002\n",
      ">> Train Epoch 64\t -- avg --\t train batch loss: -2.438033\t val batch loss: -1.180346 \t  val strategy loss: -4.922614\n",
      "EarlyStopping counter: 5 out of 25\n",
      ">> Train Epoch 65\t batch 0\t train batch loss: -2.395019\n",
      ">> Train Epoch 65\t -- avg --\t train batch loss: -2.422356\t val batch loss: -0.980916 \t  val strategy loss: -3.909579\n",
      "EarlyStopping counter: 6 out of 25\n",
      ">> Train Epoch 66\t batch 0\t train batch loss: -2.580690\n",
      ">> Train Epoch 66\t -- avg --\t train batch loss: -2.479315\t val batch loss: -1.000652 \t  val strategy loss: -4.342950\n",
      "EarlyStopping counter: 7 out of 25\n",
      ">> Train Epoch 67\t batch 0\t train batch loss: -2.243867\n",
      ">> Train Epoch 67\t -- avg --\t train batch loss: -2.483566\t val batch loss: -1.002628 \t  val strategy loss: -4.729580\n",
      "EarlyStopping counter: 8 out of 25\n",
      ">> Train Epoch 68\t batch 0\t train batch loss: -2.336735\n",
      ">> Train Epoch 68\t -- avg --\t train batch loss: -2.366612\t val batch loss: -0.965079 \t  val strategy loss: -4.256191\n",
      "EarlyStopping counter: 9 out of 25\n",
      ">> Train Epoch 69\t batch 0\t train batch loss: -2.484207\n",
      ">> Train Epoch 69\t -- avg --\t train batch loss: -2.494510\t val batch loss: -0.854224 \t  val strategy loss: -4.241448\n",
      "EarlyStopping counter: 10 out of 25\n",
      ">> Train Epoch 70\t batch 0\t train batch loss: -2.658289\n",
      ">> Train Epoch 70\t -- avg --\t train batch loss: -2.561594\t val batch loss: -0.659848 \t  val strategy loss: -3.367359\n",
      "EarlyStopping counter: 11 out of 25\n",
      ">> Train Epoch 71\t batch 0\t train batch loss: -2.270335\n",
      ">> Train Epoch 71\t -- avg --\t train batch loss: -2.394120\t val batch loss: -0.935160 \t  val strategy loss: -4.394165\n",
      "EarlyStopping counter: 12 out of 25\n",
      ">> Train Epoch 72\t batch 0\t train batch loss: -2.479051\n",
      ">> Train Epoch 72\t -- avg --\t train batch loss: -2.331477\t val batch loss: -0.778366 \t  val strategy loss: -4.082286\n",
      "EarlyStopping counter: 13 out of 25\n",
      ">> Train Epoch 73\t batch 0\t train batch loss: -2.457333\n",
      ">> Train Epoch 73\t -- avg --\t train batch loss: -2.333837\t val batch loss: -0.955929 \t  val strategy loss: -4.619027\n",
      "EarlyStopping counter: 14 out of 25\n",
      ">> Train Epoch 74\t batch 0\t train batch loss: -2.371677\n",
      ">> Train Epoch 74\t -- avg --\t train batch loss: -2.275556\t val batch loss: -0.944327 \t  val strategy loss: -3.680390\n",
      "EarlyStopping counter: 15 out of 25\n",
      ">> Train Epoch 75\t batch 0\t train batch loss: -2.539848\n",
      ">> Train Epoch 75\t -- avg --\t train batch loss: -2.423799\t val batch loss: -0.757345 \t  val strategy loss: -3.811985\n",
      "EarlyStopping counter: 16 out of 25\n",
      ">> Train Epoch 76\t batch 0\t train batch loss: -2.413346\n",
      ">> Train Epoch 76\t -- avg --\t train batch loss: -2.478415\t val batch loss: -0.833413 \t  val strategy loss: -4.101026\n",
      "EarlyStopping counter: 17 out of 25\n",
      ">> Train Epoch 77\t batch 0\t train batch loss: -2.476139\n",
      ">> Train Epoch 77\t -- avg --\t train batch loss: -2.419009\t val batch loss: -1.041859 \t  val strategy loss: -5.631938\n",
      "EarlyStopping counter: 18 out of 25\n",
      ">> Train Epoch 78\t batch 0\t train batch loss: -2.847143\n",
      ">> Train Epoch 78\t -- avg --\t train batch loss: -2.374603\t val batch loss: -0.731333 \t  val strategy loss: -3.341509\n",
      "EarlyStopping counter: 19 out of 25\n",
      ">> Train Epoch 79\t batch 0\t train batch loss: -2.493690\n",
      ">> Train Epoch 79\t -- avg --\t train batch loss: -2.351921\t val batch loss: -0.665061 \t  val strategy loss: -3.569529\n",
      "EarlyStopping counter: 20 out of 25\n",
      ">> Train Epoch 80\t batch 0\t train batch loss: -2.432819\n",
      ">> Train Epoch 80\t -- avg --\t train batch loss: -2.369760\t val batch loss: -0.689023 \t  val strategy loss: -3.481913\n",
      "EarlyStopping counter: 21 out of 25\n",
      ">> Train Epoch 81\t batch 0\t train batch loss: -2.452272\n",
      ">> Train Epoch 81\t -- avg --\t train batch loss: -2.477566\t val batch loss: -0.431227 \t  val strategy loss: -3.386463\n",
      "EarlyStopping counter: 22 out of 25\n",
      ">> Train Epoch 82\t batch 0\t train batch loss: -2.325508\n",
      ">> Train Epoch 82\t -- avg --\t train batch loss: -2.455818\t val batch loss: -0.614725 \t  val strategy loss: -2.326204\n",
      "EarlyStopping counter: 23 out of 25\n",
      ">> Train Epoch 83\t batch 0\t train batch loss: -2.294022\n",
      ">> Train Epoch 83\t -- avg --\t train batch loss: -2.383727\t val batch loss: -0.497518 \t  val strategy loss: -2.547605\n",
      "EarlyStopping counter: 24 out of 25\n",
      ">> Train Epoch 84\t batch 0\t train batch loss: -2.335961\n",
      ">> Train Epoch 84\t -- avg --\t train batch loss: -2.528216\t val batch loss: -0.495104 \t  val strategy loss: -2.068560\n",
      "EarlyStopping counter: 25 out of 25\n",
      "> Early stopping\n"
     ]
    }
   ],
   "source": [
    "best_checkpoint_path, val_loss = train(model=model, train_iter=train_iter, val_iter=val_iter,train_manager=train_manager, do_log=args.do_log, val_df=val_df)"
   ]
  },
  {
   "source": [
    "## Test loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Val loss: -6.441751\n>> Test loss: -4.840829\n"
     ]
    }
   ],
   "source": [
    "if train_manager['args']['stopping_type'] == 'strategy':\n",
    "    test_loss = evaluate_iter(model=model, data_iter=test_iter, train_manager=train_manager, do_log=False, do_strategy=True,base_df=test_df)\n",
    "else:\n",
    "    test_loss = evaluate_iter(model=model, data_iter=test_iter, train_manager=train_manager, do_log=False, do_strategy=False)\n",
    "\n",
    "print(f\">> Val loss: {val_loss:.6f}\")\n",
    "print(f\">> Test loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[ 1.11567950e+00, -1.95681203e-01, -1.33816270e-01, ...,\n",
       "          -1.13808535e+00,  7.67775147e-03,  2.40350032e+00],\n",
       "         [ 1.10041239e-01, -2.24016349e-01, -1.21578649e-01, ...,\n",
       "          -1.14331353e+00, -6.01067508e-02,  2.28199018e+00],\n",
       "         [ 6.70466110e-01, -1.92969808e-01, -1.29305755e-01, ...,\n",
       "          -1.11940434e+00, -1.06596147e-01,  2.19545070e+00],\n",
       "         ...,\n",
       "         [ 6.73579553e-01,  1.56944757e-01,  2.56580209e-01, ...,\n",
       "           1.07252786e+00,  1.22037084e+00,  2.08981886e+00],\n",
       "         [ 1.07694106e+00,  2.25401951e-01,  3.13038486e-01, ...,\n",
       "           1.15111636e+00,  1.28490407e+00,  2.14408475e+00],\n",
       "         [ 2.83211307e-01,  1.89892875e-01,  3.02244238e-01, ...,\n",
       "           1.23068953e+00,  1.35303324e+00,  2.20201345e+00]],\n",
       "\n",
       "        [[-6.30127649e-01, -6.71802831e-02, -2.10861652e-01, ...,\n",
       "          -1.18601675e+00, -1.73146586e+00, -1.78324939e+00],\n",
       "         [-4.22851950e-01, -7.87189291e-02, -2.91015962e-01, ...,\n",
       "          -1.20167858e+00, -1.74758656e+00, -1.80109369e+00],\n",
       "         [-2.07326695e-01, -3.92596007e-02, -2.88787210e-01, ...,\n",
       "          -1.22086365e+00, -1.76230594e+00, -1.81587203e+00],\n",
       "         ...,\n",
       "         [ 2.79378039e+00, -6.70463064e-02, -3.86493585e-01, ...,\n",
       "          -2.05275214e+00, -3.55084067e+00, -3.15082596e+00],\n",
       "         [ 1.87436959e+00,  6.37882760e-02, -3.94347448e-01, ...,\n",
       "          -1.94805641e+00, -3.73667178e+00, -3.34236688e+00],\n",
       "         [-2.97324235e+00, -1.63931794e-01, -4.51953271e-01, ...,\n",
       "          -2.03003207e+00, -3.96608676e+00, -3.52456615e+00]],\n",
       "\n",
       "        [[-1.94248583e-01, -2.66215276e-01, -1.45626518e-01, ...,\n",
       "          -8.49291094e-01, -6.26353812e-01, -7.97943442e-01],\n",
       "         [-7.82892932e-01, -2.24960240e-01, -1.64518765e-01, ...,\n",
       "          -9.06273242e-01, -6.70143049e-01, -8.30688520e-01],\n",
       "         [-1.59131049e+00, -3.00722119e-01, -2.04519312e-01, ...,\n",
       "          -1.01894686e+00, -7.38716606e-01, -8.75732037e-01],\n",
       "         ...,\n",
       "         [ 8.17246087e-01, -1.58358975e-01, -8.05468962e-02, ...,\n",
       "          -1.15612520e+00, -7.68930231e-01, -6.49992821e-01],\n",
       "         [ 5.05958709e-01, -1.28549909e-01, -4.82551888e-02, ...,\n",
       "          -1.06120820e+00, -7.60749060e-01, -6.58317336e-01],\n",
       "         [-8.10462783e-01, -1.45216601e-01, -6.01340436e-02, ...,\n",
       "          -1.02407270e+00, -7.73108594e-01, -6.76346050e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  3.10939718e-01, -2.19330390e-02, ...,\n",
       "           6.37696386e-01,  8.72542170e-01,  3.95013731e+00],\n",
       "         [ 1.36464079e-01,  2.26328256e-01, -3.29975110e-02, ...,\n",
       "           8.07605845e-01,  9.97623023e-01,  4.12256866e+00],\n",
       "         [-2.77398204e-01,  2.12722438e-01, -7.92947862e-03, ...,\n",
       "           9.23007674e-01,  1.09215479e+00,  4.21658062e+00],\n",
       "         ...,\n",
       "         [-9.60053280e-01,  5.39505353e-01,  3.31730067e-01, ...,\n",
       "           2.16096187e+00,  2.61697878e+00,  3.56875069e+00],\n",
       "         [ 5.04417844e-01,  6.12842116e-01,  3.08738732e-01, ...,\n",
       "           2.14253627e+00,  2.60848559e+00,  3.53103304e+00],\n",
       "         [-5.68144003e-01,  5.84302624e-01,  2.90001883e-01, ...,\n",
       "           2.10234993e+00,  2.59912738e+00,  3.50941416e+00]],\n",
       "\n",
       "        [[ 5.05988433e-01, -7.52656667e-02,  1.19699439e-01, ...,\n",
       "           6.39512139e-01,  6.85031255e-01,  3.72758259e-02],\n",
       "         [-6.55568012e-01, -1.48688055e-01,  1.15069416e-01, ...,\n",
       "           5.53737923e-01,  6.71750973e-01,  3.81544825e-02],\n",
       "         [ 1.40517075e+00, -1.48192258e-01,  1.30817305e-01, ...,\n",
       "           5.31795725e-01,  6.78020754e-01,  4.84333316e-02],\n",
       "         ...,\n",
       "         [-1.19218333e+00,  8.08019009e-02,  4.66574543e-02, ...,\n",
       "          -2.58040759e-02,  5.95791558e-01,  7.48474032e-01],\n",
       "         [-2.81694049e-01,  7.77305182e-02,  6.24723507e-02, ...,\n",
       "          -4.73196246e-02,  5.75421187e-01,  7.53063513e-01],\n",
       "         [-1.43413518e+00, -7.73575178e-02,  2.42994632e-02, ...,\n",
       "          -1.52374740e-01,  5.16332848e-01,  7.37387829e-01]],\n",
       "\n",
       "        [[-8.73946646e-01,  2.95408927e-01, -2.72861270e-01, ...,\n",
       "           1.22524640e-01, -1.18219983e+00, -1.28053531e+00],\n",
       "         [-1.46873369e+00,  2.25555701e-01, -3.25176596e-01, ...,\n",
       "           2.76085447e-02, -1.21294475e+00, -1.30479547e+00],\n",
       "         [-4.39036340e-01,  1.52207588e-01, -4.12184145e-01, ...,\n",
       "          -9.81748212e-02, -1.27523298e+00, -1.35251356e+00],\n",
       "         ...,\n",
       "         [ 1.55714090e+00,  2.32388254e-01, -4.23151755e-01, ...,\n",
       "          -1.26131670e+00, -4.13593773e+00, -2.11277648e+00],\n",
       "         [ 2.18127485e+00,  3.69327828e-01, -4.06660576e-01, ...,\n",
       "          -1.11343987e+00, -4.17138780e+00, -2.18975458e+00],\n",
       "         [-2.29179520e+00,  2.51963226e-01, -4.68452481e-01, ...,\n",
       "          -1.07227372e+00, -4.28188978e+00, -2.28366669e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 2.07193158e-01, -1.21205896e-01,  2.38282442e-02, ...,\n",
       "           3.69873786e-02, -4.02007061e-01, -6.08137384e-01],\n",
       "         [ 6.30673814e-01, -1.21898247e-01,  5.96848166e-02, ...,\n",
       "           8.74488610e-02, -3.72555382e-01, -5.92849818e-01],\n",
       "         [-1.05306074e-01, -4.07170814e-02,  4.26059047e-02, ...,\n",
       "           1.21661080e-01, -3.48267546e-01, -5.79337340e-01],\n",
       "         ...,\n",
       "         [ 5.71547302e-01, -2.39092880e-01,  6.63610021e-02, ...,\n",
       "          -2.82225751e-01,  5.50671228e-01,  3.59291508e-01],\n",
       "         [ 1.09107785e+00, -1.09134385e-01,  1.00080537e-01, ...,\n",
       "          -2.71910171e-01,  5.36096371e-01,  3.66936924e-01],\n",
       "         [ 1.52788539e+00,  5.65475461e-02,  1.58190640e-01, ...,\n",
       "          -1.46880988e-01,  5.78967576e-01,  3.97027607e-01]],\n",
       "\n",
       "        [[-1.15174765e+00, -3.26042812e-02, -2.81378100e-01, ...,\n",
       "          -8.24578314e-01, -1.30284970e+00, -1.75471442e+00],\n",
       "         [-4.79541088e-01, -5.46860859e-02, -3.26926619e-01, ...,\n",
       "          -8.82417208e-01, -1.32646846e+00, -1.77678574e+00],\n",
       "         [-4.76687901e-01, -1.46856717e-01, -3.41081756e-01, ...,\n",
       "          -9.46649201e-01, -1.35426229e+00, -1.80017354e+00],\n",
       "         ...,\n",
       "         [ 6.80462287e-02,  2.80626484e-01,  2.34060181e-02, ...,\n",
       "           1.25474970e+00, -9.22608415e-01, -2.93131744e+00],\n",
       "         [-2.59336311e-01,  2.85949243e-01,  1.69694857e-02, ...,\n",
       "           1.22990244e+00, -8.54325465e-01, -2.85705927e+00],\n",
       "         [-9.14278937e-01,  2.59172929e-01, -6.29016610e-03, ...,\n",
       "           1.12520312e+00, -8.31182013e-01, -2.82126806e+00]],\n",
       "\n",
       "        [[-1.06587515e-01, -1.43362354e-01, -3.81115775e-01, ...,\n",
       "          -1.50675330e+00, -1.20182316e+00, -5.87562149e-01],\n",
       "         [ 7.61149510e-01,  7.29297427e-02, -3.96862768e-01, ...,\n",
       "          -1.51237091e+00, -1.24147513e+00, -6.12345656e-01],\n",
       "         [ 2.14370458e+00,  1.33201867e-01, -3.58858903e-01, ...,\n",
       "          -1.46563825e+00, -1.28434901e+00, -6.43995789e-01],\n",
       "         ...,\n",
       "         [-7.62817268e-01, -1.58509891e-01, -1.50978070e-01, ...,\n",
       "          -1.52823162e+00, -2.29018126e+00, -1.72587763e+00],\n",
       "         [-9.10226588e-01, -2.17444643e-01, -1.66490119e-01, ...,\n",
       "          -1.57264278e+00, -2.27142897e+00, -1.72484214e+00],\n",
       "         [-4.00763606e-01, -2.83611430e-01, -2.06103009e-01, ...,\n",
       "          -1.60391305e+00, -2.25031080e+00, -1.72526240e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.08952080e-01, -5.24721485e-03,  6.10469710e-02, ...,\n",
       "          -1.09062378e+00, -1.88429818e+00, -2.66862656e+00],\n",
       "         [ 1.54259492e-01,  2.13629499e-02,  9.10159025e-02, ...,\n",
       "          -9.31351392e-01, -1.82374228e+00, -2.65755362e+00],\n",
       "         [-2.23675869e-02, -5.75514616e-02,  4.39644575e-02, ...,\n",
       "          -7.88975462e-01, -1.75505480e+00, -2.62906877e+00],\n",
       "         ...,\n",
       "         [-3.29847263e-01, -5.06192461e-01, -3.31567679e-01, ...,\n",
       "          -2.08560278e+00, -1.80057435e+00, -1.63297457e+00],\n",
       "         [ 1.54122157e+00, -2.34284060e-01, -2.58115663e-01, ...,\n",
       "          -1.99522525e+00, -1.80836142e+00, -1.63251499e+00],\n",
       "         [-1.50256677e+00, -2.90585424e-01, -3.63271593e-01, ...,\n",
       "          -1.97818325e+00, -1.83337659e+00, -1.62556276e+00]],\n",
       "\n",
       "        [[ 9.40226047e-01,  1.10712958e-01, -1.39767413e-01, ...,\n",
       "           5.77372036e-02,  1.94150455e-01,  1.19665555e+00],\n",
       "         [-3.96104373e-01, -5.33979646e-02, -1.35945862e-01, ...,\n",
       "           1.15994739e-01,  2.30065568e-01,  1.29313533e+00],\n",
       "         [ 1.98697411e-01,  3.09501940e-02, -9.17691443e-02, ...,\n",
       "           1.87659077e-01,  2.66893354e-01,  1.35511534e+00],\n",
       "         ...,\n",
       "         [-6.31893808e-01,  2.94843974e-01,  2.83961026e-01, ...,\n",
       "           2.13712197e+00,  1.54682827e+00,  1.06731363e+00],\n",
       "         [-4.74507642e-01,  2.73137730e-01,  2.44870534e-01, ...,\n",
       "           2.06708369e+00,  1.56784851e+00,  1.08376310e+00],\n",
       "         [-9.65207720e-01,  1.72652658e-01,  2.22843497e-01, ...,\n",
       "           1.97031071e+00,  1.58190150e+00,  1.10166201e+00]],\n",
       "\n",
       "        [[ 7.70588851e-01,  6.40430929e-01,  3.61325727e-01, ...,\n",
       "           1.19971616e+00,  8.99767483e-01,  4.46550120e-01],\n",
       "         [ 5.78347355e-01,  6.00064962e-01,  3.75861170e-01, ...,\n",
       "           1.23541100e+00,  9.21209961e-01,  4.59456930e-01],\n",
       "         [-2.11696130e-02,  5.48952190e-01,  3.56889941e-01, ...,\n",
       "           1.25618296e+00,  9.40203858e-01,  4.72338609e-01],\n",
       "         ...,\n",
       "         [ 6.86143533e-03, -1.00017611e-01, -5.45402985e-02, ...,\n",
       "          -8.43096500e-01,  7.54207343e-02,  9.80981702e-01],\n",
       "         [-4.44618521e-01, -1.25510178e-01, -7.76190317e-02, ...,\n",
       "          -8.35683863e-01,  4.13878345e-02,  9.54365433e-01],\n",
       "         [-4.00479602e-01, -2.31651518e-01, -7.65746176e-02, ...,\n",
       "          -8.37672137e-01,  3.47408405e-03,  9.20636232e-01]]],\n",
       "\n",
       "\n",
       "       [[[-4.41644216e-01, -1.94166540e-01,  1.75763501e-02, ...,\n",
       "          -6.20777972e-01, -4.41473690e-03, -7.55185127e-03],\n",
       "         [-3.61463493e-01, -2.54946609e-01,  1.62406419e-02, ...,\n",
       "          -8.61935853e-01, -1.28314596e-01, -5.43942531e-02],\n",
       "         [-5.53035425e-01, -3.02940207e-01, -4.56725167e-02, ...,\n",
       "          -1.06607716e+00, -2.52421502e-01, -1.02760073e-01],\n",
       "         ...,\n",
       "         [-4.69207098e-01,  1.59424063e-01,  3.04319620e-01, ...,\n",
       "           8.80469933e-01,  1.36302532e+00,  1.38771100e+00],\n",
       "         [ 1.83137347e+00,  3.45539783e-01,  4.00639102e-01, ...,\n",
       "           9.25726476e-01,  1.38990085e+00,  1.43841593e+00],\n",
       "         [ 1.74041760e+00,  4.60179484e-01,  5.38908293e-01, ...,\n",
       "           1.03157069e+00,  1.45164925e+00,  1.51003852e+00]],\n",
       "\n",
       "        [[ 5.12284885e-02, -2.31672924e-01,  1.64428613e-01, ...,\n",
       "           1.91624287e-01,  1.27820603e+00,  1.84641085e+00],\n",
       "         [ 1.14486272e+00, -1.62194006e-01,  2.21800507e-01, ...,\n",
       "           1.45054111e-01,  1.27982744e+00,  1.89780830e+00],\n",
       "         [-5.23040952e-01, -1.87559454e-01,  2.02722519e-01, ...,\n",
       "           7.62824088e-02,  1.27593243e+00,  1.95162142e+00],\n",
       "         ...,\n",
       "         [ 3.30337764e-01, -1.00606131e-01, -6.34058854e-02, ...,\n",
       "          -1.04679981e+00, -6.45606822e-01,  1.02384805e+00],\n",
       "         [ 7.71788632e-01, -4.00741032e-02, -4.08708790e-02, ...,\n",
       "          -9.28829539e-01, -6.22737160e-01,  1.01505894e+00],\n",
       "         [ 1.30216580e-01, -7.33332451e-02, -3.33098424e-02, ...,\n",
       "          -8.14211649e-01, -5.96396651e-01,  1.00658922e+00]],\n",
       "\n",
       "        [[ 2.89517932e+00,  4.97522883e-01,  3.27942035e-01, ...,\n",
       "           1.39095489e+00,  7.48508556e-01, -1.30498266e+00],\n",
       "         [-5.62425898e-01,  3.80477472e-01,  2.81423326e-01, ...,\n",
       "           1.64794589e+00,  8.85321943e-01, -1.12455249e+00],\n",
       "         [ 1.14051477e-01,  3.98983432e-01,  2.83319946e-01, ...,\n",
       "           1.84106697e+00,  1.00278425e+00, -9.70499171e-01],\n",
       "         ...,\n",
       "         [-2.28204537e-01, -1.20900779e-01,  2.70691958e-01, ...,\n",
       "           4.74786167e-01,  1.19670025e+00,  9.61304009e-01],\n",
       "         [ 2.44421212e-01, -1.18094787e-01,  2.62560169e-01, ...,\n",
       "           4.35518707e-01,  1.19911506e+00,  9.76104974e-01],\n",
       "         [ 1.12066295e-01, -9.47728525e-02,  2.55142489e-01, ...,\n",
       "           4.04536178e-01,  1.20329868e+00,  9.91528507e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.16870075e+00,  1.70558662e-02,  4.13696693e-02, ...,\n",
       "           1.17402077e+00,  4.18385004e+00,  5.46653080e+00],\n",
       "         [-5.03213154e-02,  7.82546958e-02,  3.78955804e-02, ...,\n",
       "           1.25423863e+00,  4.15128533e+00,  5.34110647e+00],\n",
       "         [ 1.68785101e+00,  3.24182392e-01,  1.34475933e-01, ...,\n",
       "           1.48328156e+00,  4.16058346e+00,  5.18252947e+00],\n",
       "         ...,\n",
       "         [-3.89379301e-01,  5.47453951e-01,  4.87403628e-01, ...,\n",
       "           2.35961771e+00,  3.61736069e+00,  3.44336703e+00],\n",
       "         [ 1.50677662e+00,  6.51820516e-01,  4.93308835e-01, ...,\n",
       "           2.41007878e+00,  3.63865541e+00,  3.43097684e+00],\n",
       "         [ 2.13521041e+00,  7.77110389e-01,  5.63800592e-01, ...,\n",
       "           2.50404910e+00,  3.67210258e+00,  3.41016777e+00]],\n",
       "\n",
       "        [[ 7.62208398e-01, -9.94794231e-02, -5.34957908e-01, ...,\n",
       "          -1.41034100e+00, -3.82950730e+00, -5.57434136e+00],\n",
       "         [-1.03888737e+00, -2.26313973e-01, -6.18760315e-01, ...,\n",
       "          -1.40304524e+00, -3.88970780e+00, -5.73472066e+00],\n",
       "         [-9.62798216e-01, -2.15274655e-01, -6.92691764e-01, ...,\n",
       "          -1.45152419e+00, -4.00382407e+00, -5.94551036e+00],\n",
       "         ...,\n",
       "         [-9.62037744e-01, -1.28450081e-01, -4.43063261e-01, ...,\n",
       "          -2.20731123e+00, -3.84955546e+00, -3.80602565e+00],\n",
       "         [-2.39344935e+00, -1.75527865e-01, -5.75794418e-01, ...,\n",
       "          -2.35840409e+00, -3.87387441e+00, -3.77410508e+00],\n",
       "         [ 4.63600526e-01, -1.59614847e-01, -5.88319588e-01, ...,\n",
       "          -2.47574905e+00, -3.92120032e+00, -3.77855207e+00]],\n",
       "\n",
       "        [[ 2.85600356e-01,  1.23489937e-01,  3.29688105e-01, ...,\n",
       "           1.12240692e+00,  1.48845815e+00,  1.17522118e+00],\n",
       "         [ 2.11018094e+00,  2.40589411e-01,  4.50183557e-01, ...,\n",
       "           1.21541405e+00,  1.53723185e+00,  1.20132423e+00],\n",
       "         [ 4.92929884e-01,  2.25084208e-01,  4.73741321e-01, ...,\n",
       "           1.32203785e+00,  1.59135709e+00,  1.22900740e+00],\n",
       "         ...,\n",
       "         [-1.91504644e-01, -2.41788909e-01,  3.02772733e-01, ...,\n",
       "           1.29848956e+00,  2.23352519e+00,  1.39536593e+00],\n",
       "         [ 2.29209636e+00, -9.30908548e-02,  4.59273239e-01, ...,\n",
       "           1.33922535e+00,  2.27944707e+00,  1.42021553e+00],\n",
       "         [-1.15439254e+00, -1.02041114e-01,  4.47282576e-01, ...,\n",
       "           1.28964037e+00,  2.31738605e+00,  1.45498001e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 1.03025963e+00, -2.65729361e-02,  1.27190573e-01, ...,\n",
       "          -5.46601422e-02, -2.40309530e+00, -2.71905714e+00],\n",
       "         [ 1.26706097e+00,  7.94093789e-02,  1.65275452e-01, ...,\n",
       "           1.76035769e-01, -2.12289733e+00, -2.66020764e+00],\n",
       "         [ 5.04391753e-02,  7.02683232e-02,  1.70649920e-01, ...,\n",
       "           3.70486185e-01, -1.87268178e+00, -2.61699821e+00],\n",
       "         ...,\n",
       "         [ 1.18900761e+00,  5.17588877e-01,  3.23900603e-01, ...,\n",
       "           1.77372468e+00,  1.52554846e+00,  3.45565149e-01],\n",
       "         [-8.24517004e-01,  3.46438665e-01,  2.96899067e-01, ...,\n",
       "           1.73052912e+00,  1.52544390e+00,  3.69174289e-01],\n",
       "         [ 5.87350676e-01,  3.50971802e-01,  2.83159143e-01, ...,\n",
       "           1.69005552e+00,  1.51833160e+00,  3.90415001e-01]],\n",
       "\n",
       "        [[-2.67456447e-01,  1.45637840e-01, -1.26242034e-01, ...,\n",
       "          -3.69547323e-01, -6.20756591e-01, -6.85904161e-01],\n",
       "         [ 3.77428297e-01,  1.33769867e-01, -1.29171798e-01, ...,\n",
       "          -3.01689467e-01, -5.97894447e-01, -6.86088356e-01],\n",
       "         [ 1.46033361e+00,  2.23193524e-01, -9.57524603e-02, ...,\n",
       "          -1.91054754e-01, -5.57156608e-01, -6.79780217e-01],\n",
       "         ...,\n",
       "         [-2.43606534e+00, -3.17114606e-01, -1.55675086e-01, ...,\n",
       "          -1.44853184e+00, -1.12095681e+00, -1.47207209e+00],\n",
       "         [ 3.06492611e-01, -3.76476512e-01, -1.84006078e-01, ...,\n",
       "          -1.57782742e+00, -1.14051330e+00, -1.39220627e+00],\n",
       "         [-3.09303267e-01, -3.92259075e-01, -2.02846470e-01, ...,\n",
       "          -1.66834900e+00, -1.15912153e+00, -1.32900296e+00]],\n",
       "\n",
       "        [[-6.15608696e-01, -2.12380089e-01,  1.32879403e-01, ...,\n",
       "           1.74154010e-01,  2.95243954e+00,  4.28932508e+00],\n",
       "         [-1.01387444e+00, -2.98016264e-01,  8.75239003e-02, ...,\n",
       "          -1.84985042e-01,  2.96398626e+00,  4.51968529e+00],\n",
       "         [ 1.56809630e-01, -2.73991741e-01,  4.27539759e-02, ...,\n",
       "          -4.95397557e-01,  2.84287006e+00,  4.53782779e+00],\n",
       "         ...,\n",
       "         [-1.56966148e-01, -7.92567506e-02, -1.81369447e-01, ...,\n",
       "          -6.28757151e-01, -1.19378167e+00, -3.96676844e-01],\n",
       "         [ 2.66161275e-01,  6.09087503e-02, -1.87414928e-01, ...,\n",
       "          -5.82839031e-01, -1.20023320e+00, -4.13862781e-01],\n",
       "         [-1.78055263e+00,  1.22710596e-02, -2.25180834e-01, ...,\n",
       "          -6.22415333e-01, -1.23675650e+00, -4.41267689e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00, -1.36320750e-01, -3.49932466e-02, ...,\n",
       "          -7.14436017e-01, -5.71862351e-01,  9.99013467e-01],\n",
       "         [ 5.84421719e-01,  1.38107001e-01, -5.02862647e-02, ...,\n",
       "          -5.92501777e-01, -5.39955443e-01,  1.01294928e+00],\n",
       "         [ 0.00000000e+00,  1.49626256e-01, -3.93685777e-02, ...,\n",
       "          -4.81843261e-01, -5.06123008e-01,  1.02021390e+00],\n",
       "         ...,\n",
       "         [ 2.85175074e-01,  1.88035021e-02,  2.08167797e-02, ...,\n",
       "           1.92054464e-01,  1.54430926e-01,  8.71844833e-01],\n",
       "         [ 9.43605398e-01, -4.40234919e-03,  2.15794001e-02, ...,\n",
       "           1.79341221e-01,  1.51496046e-01,  8.68584219e-01],\n",
       "         [-6.87670646e-01, -1.26782415e-02, -1.73441273e-03, ...,\n",
       "           1.24551183e-01,  1.30673924e-01,  8.54304594e-01]],\n",
       "\n",
       "        [[-5.21397919e-01,  2.77271344e-02,  9.40113898e-02, ...,\n",
       "           9.58852530e-01,  6.69759644e-01, -6.58626734e-02],\n",
       "         [ 1.32270660e-01,  9.89032125e-02,  9.34703250e-02, ...,\n",
       "           9.10183947e-01,  6.68694114e-01, -5.68145145e-02],\n",
       "         [-4.02357219e-01,  5.01247229e-02,  8.13099883e-02, ...,\n",
       "           8.47356240e-01,  6.60824937e-01, -4.97589534e-02],\n",
       "         ...,\n",
       "         [-1.01620482e+00, -1.93996667e-01, -2.15989533e-01, ...,\n",
       "          -8.97883242e-01, -1.46599958e+00, -1.00620953e+00],\n",
       "         [-1.66509973e+00, -2.19870589e-01, -2.46003731e-01, ...,\n",
       "          -9.34925372e-01, -1.50679760e+00, -1.04945116e+00],\n",
       "         [ 1.01192787e+00,  0.00000000e+00, -2.38529447e-01, ...,\n",
       "          -9.39824247e-01, -1.54297630e+00, -1.09774746e+00]],\n",
       "\n",
       "        [[-9.93253502e-01, -2.91073876e-02,  1.35449181e-01, ...,\n",
       "           2.05876409e-01, -1.02194645e-02, -1.46901313e+00],\n",
       "         [-1.01054657e+00, -9.40256146e-02,  1.01031989e-01, ...,\n",
       "           6.45736460e-02, -5.98560064e-02, -1.52649688e+00],\n",
       "         [-8.04674832e-01, -3.00197050e-01,  7.17218425e-02, ...,\n",
       "          -1.11586579e-01, -1.25297372e-01, -1.58255563e+00],\n",
       "         ...,\n",
       "         [-3.03707682e-01, -1.18888928e-01, -1.31562683e-01, ...,\n",
       "          -5.76069645e-01, -1.58790158e+00, -3.41624396e+00],\n",
       "         [-4.03262637e-01, -7.80003018e-02, -1.52494868e-01, ...,\n",
       "          -6.55609362e-01, -1.73445654e+00, -3.69246074e+00],\n",
       "         [-2.98070775e+00, -4.03373122e-01, -2.23956087e-01, ...,\n",
       "          -1.05983088e+00, -1.95870195e+00, -3.87794628e+00]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "pickle.load(open('input_numpy.p', 'rb'))"
   ]
  }
 ]
}